5. 
    1.  R and W are the number of nodes needed to complete successful read and write operations, respectively, and N is the number of nodes to which data is replicated in a Dynamo instance. Assuming no hinted handoff, the maximum number of node failures that could be absorbed while still returning consistent data during a read operation is N - R.

    2. Eventual consistency means that all copies of data in the data will eventually be consistent, so that reads of the data will eventually always return the same result. This is a design choice that prioritizes availability over consistency. Dynamo always allows writes and updates of data without waiting to synchronize all copies of the data between writes. This allows users of the api to edit their data without ever having to wait. However, this also means that subsequent reads of the data could be inconsistent in the short term. A specific example used in the article is the Amazon shopping cart - adding items to the shopping cart is a write action, so here amazon prioritized allowing users to add items to the cart with minimal delay. However, if the user then wants to review all items in the cart, causing a read action, it is possible that the specific copy of the data being read won't have been updated yet with the new items. Also, under the conditions of node failures or network partitions, divergent branches of data history could be generated. Dynamo can handle the reconciliation of some of these cases internally, but more complex cases must be reconciled by the application. In the case of the shopping cart, in unioning of two divergent item lists, added items will never be lost, but items deleted in only one of the lists could resurface.
    
    3. In a hash(key) % N system, where N represents the number of nodes, if you lose a node, that will throw off the calculation of where data for a given key should be stored - all (or most of) the data will have to be resorted among the remaining nodes for it to be addressed correctly. In a consistent hashing system, the hash output space is circular, and nodes are responsible for data stored at keys within a specific region on the circle. That way, if you lose a node, the node after it in the circle will become responsible for replicating that lost nodes data, and some of the data that was replicated onto the lost node will be replicated onto the following node. In this way, only the lost node's immediate neighbors are affected.